<!doctype html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
	<meta charset="utf-8">

	<title>Research | Yongtao Hu</title>

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<script src="/js/vendor/modernizr-2.5.3.min.js"></script>

	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	<script>window.jQuery || document.write('<script src="/js/vendor/jquery-1.7.2.min.js"><\/script>')</script>

	<script src="/js/vendor/spamspan.min.js"></script>
	<script src="/js/vendor/prettify.js"></script>

	<link rel="stylesheet" href="/plugins/social-media-widget/social_widget.css?ver=3.9.1">
	<link rel="stylesheet" href="/glyphicons/css/glyphicons.css">
	<link rel="stylesheet" href="/css/bootstrap.css">
	<link rel="stylesheet" href="/css/bootstrap-responsive.css">
	<link rel="stylesheet" href="/css/app.css">
	<script type='text/javascript' src='/js/plugins.js'></script>
	<script type='text/javascript' src='/js/main.js'></script>
	<link rel="canonical" href="http://herohuyongtao.github.io/">
	<link rel="shortcut icon" href="http://herohuyongtao.github.io/images/logo.ico">

	<!-- to toggle text -->
	<style type="text/css">
		a.toggle_text_link {
			cursor:pointer;
		}

		pre.invisible_text {
			display: none;
		}
	</style>
	<script language="javascript" type="text/javascript">
		function toggle(element) {
			if(element.style.display=="block") {
				element.style.display="none";
			} else {
				element.style.display="block";
			}
		}
	</script>
</head>

<body class="page page-id-25 page-parent page-template-default top-navbar">
  <!--[if lt IE 7]><div class="alert">Your browser is <em>ancient!</em> <a href="http://browsehappy.com/">Upgrade to a different browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to experience this site.</div><![endif]-->

    <header id="banner" class="navbar navbar-fixed-top" role="banner">
		<div class="navbar-inner">
			<div class="container">
				<a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</a>
				<a class="brand" href="http://herohuyongtao.github.io/">
					Yongtao Hu
				</a>
				<nav id="nav-main" class="nav-collapse" role="navigation">
					<ul class="nav">
						<li class="menu-home"><a href="/">Home</a></li>
						<li class="menu-publications active active"><a href="/publications/">Publications</a></li>
						<li class="menu-projects"><a href="/projects/">Projects</a></li>
						<li class="menu-cv"><a href="/cv/">CV</a></li>
						<li class="menu-news"><a href="/news/">News</a></li>
						<li class="menu-links"><a href="/links/">Links</a></li>
					</ul>
				</nav>
			</div>
		</div>
	</header>

    <div id="wrap" class="container" role="document">
		<div id="content" class="row">
			<!-- publications div -->
			<div id="main" class="span12" role="main">
				<div class="page-header">
					<h1>Publications</h1>
				</div>

				<!-- google scholar & dblp -->
				<div align="center">
					<img src="/images/scholar.png" width="15" height="15" /> <a href="https://scholar.google.com.hk/citations?user=uaKgQSEAAAAJ&hl=en">Google Scholar</a>
					&nbsp;&nbsp;&middot;&nbsp;&nbsp;
					<img src="/images/dblp.png" width="15" height="15" /> <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Yongtao">DBLP</a>
				</div>

				<!-- 2016 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2016</h2>
					</div>
				</div>

				<!-- 16'AAAI look-listen-learn -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/publications/look-listen-learn/">
							<img class="dropshadow pull-right" alt="" src="/publications/look-listen-learn/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/publications/look-listen-learn/">
							<strong>Look, Listen and Learn - A Multimodal LSTM for Speaker Identiﬁcation</strong>
						</a>
						<br />
						<a href="http://www.jimmyren.com/">Jimmy SJ. Ren</a>,
						<b>Yongtao Hu</b>,
						<a href="http://gdriv.es/yuwing">Yu-Wing Tai</a>,
						<a href="http://i.cs.hku.hk/~cwang/">Chuan Wang</a>,
						<a href="http://lxu.me/">Li Xu</a>,
						<a href="https://www.linkedin.com/in/wenxiu-sun-bb6b292b">Wenxiu Sun</a>, and
						<a href="http://www.yan-qiong.com/">Qiong Yan</a>
						<br />
						<em>The 30th AAAI Conference on Artificial Intelligence (<b>AAAI 2016</b>)</em>
						<br />
						<em>"speaker identification through multimodal weight sharing LSTM"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/publications/look-listen-learn/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/publications/look-listen-learn/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(ren2016look);">abstract</a>
								<pre id="ren2016look" class="invisible_text">Speaker identiﬁcation refers to the task of localizing the face of a person who has the same identity as the ongoing voice in a video. This task not only requires collective perception over both visual and auditory signals, the robustness to handle severe quality degradations and unconstrained content variations are also indispensable. In this paper, we describe a novel multimodal Long Short-Term Memory (LSTM) architecture which seamlessly uniﬁes both visual and auditory modalities from the beginning of each sequence input. The key idea is to extend the conventional LSTM by not only sharing weights across time steps, but also sharing weights across modalities. We show that modeling the temporal dependency across face and voice can signiﬁcantly improve the robustness to content quality degradations and variations. We also found that our multimodal LSTM is robustness to distractors, namely the non-speaking identities. We applied our multimodal LSTM to The Big Bang Theory dataset and showed that our system outperforms the state-of-the-art systems in speaker identification with lower false alarm rate and higher recognition accuracy.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-turtle"></em> <a href="https://github.com/jimmy-ren/lstm_speaker_naming_aaai16">source code & dataset</a></li>
							<li><em class="icon-turtle"></em> <a href="https://github.com/jimmy-ren/vLSTM">more applications</a></li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(ren2016look2);">bibtex</a>
								<pre id="ren2016look2" class="invisible_text">
@inproceedings{ren2016look,
  title={{Look, Listen and Learn - A Multimodal LSTM for Speaker Identification}},
  author={Ren, Jimmy SJ. and Hu, Yongtao and Tai, Yu-Wing and Wang, Chuan and Xu, Li and Sun, Wenxiu and Yan, Qiong},
  booktitle={Proceedings of the 30th AAAI Conference on Artificial Intelligence},
  pages={xxxx--xxxx},
  year={2016}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> DOI</li>
						</ul>
					</div>
				</div>

				<!-- 2015 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2015</h2>
					</div>
				</div>

				<!-- 15'MM speaker-naming -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/publications/speaker-naming/">
							<img class="dropshadow pull-right" alt="" src="/publications/speaker-naming/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/publications/speaker-naming/">
							<strong>Deep Multimodal Speaker Naming</strong>
						</a>
						<br />
						<b>Yongtao Hu</b>,
						<a href="http://www.jimmyren.com/">Jimmy SJ. Ren</a>,
						<a href="http://jwdai.net/">Jingwen Dai</a>,
						<a href="http://iris.usc.edu/Vision-Users/OldUsers/cyuan/">Chang Yuan</a>,
						<a href="http://lxu.me/">Li Xu</a>, and
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br />
						<em>The 23rd Annual ACM International Conference on Multimedia (<b>MM 2015</b>)</em>
						<br />
						<em>"realtime speaker naming through CNN based deeply learned face-audio fusion"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/publications/speaker-naming/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/publications/speaker-naming/paper.pdf">paper</a></li>
							<li><em class="icon-picture"></em> <a href="/publications/speaker-naming/poster.pdf">poster</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2015speakernaming);">abstract</a>
								<pre id="hu2015speakernaming" class="invisible_text">Automatic speaker naming is the problem of localizing as well as identifying each speaking character in a TV/movie/live show video. This is a challenging problem mainly attributes to its multimodal nature, namely face cue alone is insufficient to achieve good performance. Previous multimodal approaches to this problem usually process the data of different modalities individually and merge them using handcrafted heuristics. Such approaches work well for simple scenes, but fail to achieve high performance for speakers with large appearance variations. In this paper, we propose a novel convolutional neural networks (CNN) based learning framework to automatically learn the fusion function of both face and audio cues. We show that without using face tracking, facial landmark localization or subtitle/transcript, our system with robust multimodal feature extraction is able to achieve state-of-the-art speaker naming performance evaluated on two diverse TV series. The dataset and implementation of our algorithm are publicly available online.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-briefcase"></em> <a href="/publications/speaker-naming/dataset">dataset</a></li>
							<li><em class="icon-turtle"></em> <a href="https://bitbucket.org/herohuyongtao/mm15-speaker-naming">source code</a></li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2015speakernaming2);">bibtex</a>
								<pre id="hu2015speakernaming2" class="invisible_text">
@inproceedings{hu2015deep,
  title={{Deep Multimodal Speaker Naming}},
  author={Hu, Yongtao and Ren, Jimmy SJ. and Dai, Jingwen and Yuan, Chang and Xu, Li and Wang, Wenping},
  booktitle={Proceedings of the 23rd Annual ACM International Conference on Multimedia},
  pages={1107--1110},
  year={2015},
  organization={ACM}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="http://dx.doi.org/10.1145/2733373.2806293">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- 15'TMM video2comics -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/publications/video2comics/">
							<img class="dropshadow pull-right" alt="" src="/publications/video2comics/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/publications/video2comics/">
							<strong>Content-Aware Video2Comics with Manga-Style Layout</strong>
						</a>
						<br />
						<a href="https://www.linkedin.com/in/guangmei-jing-04917a49">Guangmei Jing</a>,
						<b>Yongtao Hu</b>,
						<a href="http://cs.nju.edu.cn/ywguo/">Yanwen Guo</a>,
						<a href="http://i.cs.hku.hk/~yzyu/">Yizhou Yu</a>, and
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br />
						<em>IEEE Transactions on Multimedia (<b>TMM 2015</b>)</em>
						<br />
						<em>"auto-convert conversational videos into comics with manga-style layout"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/publications/video2comics/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/publications/video2comics/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(jing2015content_abstract);">abstract</a>
								<pre id="jing2015content_abstract" class="invisible_text">We introduce in this paper a new approach that conveniently converts conversational videos into comics with manga-style layout. With our approach, the manga-style layout of a comic page is achieved in a content-driven manner, and the main components, including panels and word balloons, that constitute a visually pleasing comic page are intelligently organized. Our approach extracts key frames on speakers by using a speaker detection technique such that word balloons can be placed near the corresponding speakers. We qualitatively measure the information contained in a comic page. With the initial layout automatically determined, the final comic page is obtained by maximizing such a measure and optimizing the parameters relating to the optimal display of comics. An efficient Markov chain Monte Carlo sampling algorithm is designed for the optimization. Our user study demonstrates that users much prefer our manga-style comics to purely Western style comics. Extensive experiments and comparisons against previous work also verify the effectiveness of our approach.</pre>
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(jing2015content_bibtex);">bibtex</a>
								<pre id="jing2015content_bibtex" class="invisible_text">
@article{jing2015sontent,
  title={{Content-Aware Video2Comics with Manga-Style Layout}},
  author={Jing, Guangmei and Hu, Yongtao and Guo, Yanwen and Yu, Yizhou and Wang, Wenping},
  journal={IEEE Transactions on Multimedia (TMM)},
  volume={17},
  number={12},
  pages={2122--2133},
  year={2015},
  publisher={IEEE}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="http://dx.doi.org/10.1109/TMM.2015.2474263">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- 2014 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2014</h2>
					</div>
				</div>

				<!-- 14'TOMM smart-subtitle -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/publications/speaker-following-subtitles/">
							<img class="dropshadow pull-right" alt="" src="/publications/speaker-following-subtitles/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/publications/speaker-following-subtitles/">
							<strong>Speaker-Following Video Subtitles</strong>
						</a>
						<br />
						<b>Yongtao Hu</b>,
						<a href="http://jankautz.com/">Jan Kautz</a>,
						<a href="http://i.cs.hku.hk/~yzyu/">Yizhou Yu</a>, and
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br />
						<em>ACM Transactions on Multimedia Computing, Communications, and Applications (<b>TOMM 2014</b>)</em>
						<br />
						<em>"auto-generate speaker-following subtitles to enhance video viewing experience"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/publications/speaker-following-subtitles/">project page</a></li>
							<li><em class="icon-file"></em> <a href="http://arxiv.org/pdf/1407.5145v1.pdf">arXiv (with color)</a></li>
							<li><em class="icon-file"></em> <a href="/publications/speaker-following-subtitles/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014speakerfollowing2);">abstract</a>
								<pre id="hu2014speakerfollowing2" class="invisible_text">We propose a new method for improving the presentation of subtitles in video (e.g. TV and movies). With conventional subtitles, the viewer has to constantly look away from the main viewing area to read the subtitles at the bottom of the screen, which disrupts the viewing experience and causes unnecessary eyestrain. Our method places on-screen subtitles next to the respective speakers to allow the viewer to follow the visual content while simultaneously reading the subtitles. We use novel identification algorithms to detect the speakers based on audio and visual information. Then the placement of the subtitles is determined using global optimization. A comprehensive usability study indicated that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014speakerfollowing);">bibtex</a>
								<pre id="hu2014speakerfollowing" class="invisible_text">
@article{hu2014speaker,
  title={{Speaker-Following Video Subtitles}},
  author={Hu, Yongtao and Kautz, Jan and Yu, Yizhou and Wang, Wenping},
  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  volume={11},
  number={2},
  pages={32:1--32:17},
  year={2014},
  publisher={ACM}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="http://dx.doi.org/10.1145/2632111">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- thesis -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>Ph.D. Dissertation</h2>
					</div>
				</div>

				<!-- phd thesis -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/publications/thesis/thumbnail.png" width="150" height="90" />
					</div>
					<div class="span8">
						<a href="">
							<strong>Multimodal Speaker Localization and Identiﬁcation for Video Processing</strong>
						</a>
						<br />
						<b>Yongtao Hu</b>
						<br />
						<em>The University of Hong Kong, December 2014</em>
						<br />
						<em>"a more-or-less summary of the works published at TOMM'14, TMM'15 and MM'15"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-file"></em> paper</li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014thesis2);">abstract</a>
								<pre id="hu2014thesis2" class="invisible_text">
With the rapid growth of the multimedia data, especially for videos, the ability to better and time-efficiently understand them is becoming increasingly important. For videos, speakers, which are normally what our eyes are focused on, have played a key role to understand the content. With the detailed information of the speakers like their positions and identities, many high-level video processing/analysis tasks, such as semantic indexing, retrieval summarization. Recently, some multimedia content providers, such as Amazon/IMDb and Google Play, had the ability to provide additional cast and characters information for movies and TV series during playback, which can be achieved via a combination of face tracking, automatic identification and crowd sourcing. The main topics includes speaker localization, speaker identification, speech recognition, etc.
<br />This thesis first investigates the problem of speaker localization. A new algorithm for effectively detecting and localizing speakers based on multimodal visual and audio information is presented. We introduce four new features for speaker detection and localization, including lip motion, center contribution, length consistency and audio-visual synchrony, and combine them in a cascade model. Experiments on several movies and TV series indicate that, all together, they improve the speaker detection and localization accuracy by 7.5%--20.5%. Based on the locations of speakers, an efficient optimization algorithm for determining appropriate locations to place subtitles is proposed. This further enables us to develop an automatic end-to-end system for subtitle placement for TV series and movies.
<br />The second part of this thesis studies the speaker identification problem in videos. We propose a novel convolutional neural networks (CNN) based learning framework to automatically learn the fusion function of both faces and audio cues. A systematic multimodal dataset with face and audio samples collected from the real-life videos is created. The high variation of the samples in the dataset, including pose, illumination, facial expression, accessory, occlusion, image quality, scene and aging, wonderfully approximates the realistic scenarios and allows us to fully explore the potential of our method in practical applications. Extensive experiments on our new multi-modal dataset show that our method achieves state-of-the-art performance (over 90%) in speaker naming task without using face/person tracking, facial landmark localization or subtitle/transcript, thus making it suitable for real-life applications.
<br />The speaker-oriented techniques presented in this thesis have lots of applications for video processing. Through extensive experimental results on multiple real-life videos including TV series, movies and online video clips, we demonstrate the ability to extend our previous multimodal speaker localization and speaker identification algorithms in video processing tasks. Particularly, three main categories of applications are introduced, including (1) combine applying our speaker-following video subtitles and speaker naming work to enhance video viewing experience, where a comprehensive usability study with 219 users verifies that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain; (2) automatically convert a video sequence into comics based on our speaker localization algorithms; and (3) extend our speaker naming work to handle real-life video summarization tasks.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014thesis);">bibtex</a>
								<pre id="hu2014thesis" class="invisible_text">
@phdthesis{hu2014thesis,
  title={{Multimodal Speaker Localization and Identification for Video Processing}},
  author={Hu, Yongtao},
  year={2014},
  month={12},
  school={The University of Hong Kong}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
				<p>&nbsp;</p>
			</div><!-- /#main -->
		</div><!-- /#content -->
    </div><!-- /#wrap -->

	<footer id="content-info" class="container" role="contentinfo">
		<table width="100%">
			<tr>
				<td>
					<p class="copy"><small>&copy; 2014-2016 Yongtao Hu</small></p>
				</td>
				<td style="text-align:center">
					<p class="copy"><small>Last updated on 03/11/2016</small></p>
				</td>
				<td style="text-align:right">
					<a href="http://SiteStates.com" title="Site access statistics">
						<img src="http://SiteStates.com/show/image/31185.jpg" border="0" />
					</a>
				</td>
			</tr>
		</table>
	</footer>

	<!-- From http://stackoverflow.com/a/11668413/72470 -->
	<script>
	  !function ($) {
		$(function(){
		  window.prettyPrint && prettyPrint()
		})
	  }(window.jQuery)
	</script>

</body>
</html>
