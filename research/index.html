<!doctype html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
	<meta charset="utf-8">

	<title>Research | Yongtao Hu</title>

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<script src="/js/vendor/modernizr-2.5.3.min.js"></script>

	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	<script>window.jQuery || document.write('<script src="/js/vendor/jquery-1.7.2.min.js"><\/script>')</script>

	<script src="/js/vendor/spamspan.min.js"></script>
	<script src="/js/vendor/prettify.js"></script>

	<link rel="stylesheet" href="/plugins/social-media-widget/social_widget.css?ver=3.9.1">
	<link rel="stylesheet" href="/glyphicons/css/glyphicons.css">
	<link rel="stylesheet" href="/css/bootstrap.css">
	<link rel="stylesheet" href="/css/bootstrap-responsive.css">
	<link rel="stylesheet" href="/css/app.css">
	<script type='text/javascript' src='/js/plugins.js'></script>
	<script type='text/javascript' src='/js/main.js'></script>
	<link rel="canonical" href="http://herohuyongtao.github.io/research/">
	<link rel="shortcut icon" href="http://herohuyongtao.github.io/images/logo.ico">

	<!-- to toggle text -->
	<style type="text/css">
		a.toggle_text_link {
			cursor:pointer;
		}

		pre.invisible_text {
			display: none;
		}
	</style>
	<script language="javascript" type="text/javascript">
		function toggle(element) {
			if(element.style.display=="block") {
				element.style.display="none";
			} else {
				element.style.display="block";
			}
		}
	</script>
</head>

<body class="page page-id-25 page-parent page-template-default top-navbar">
  <!--[if lt IE 7]><div class="alert">Your browser is <em>ancient!</em> <a href="http://browsehappy.com/">Upgrade to a different browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to experience this site.</div><![endif]-->

    <header id="banner" class="navbar navbar-fixed-top" role="banner">
		<div class="navbar-inner">
			<div class="container">
				<a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</a>
				<a class="brand" href="http://herohuyongtao.github.io/">
					Yongtao Hu
				</a>
				<nav id="nav-main" class="nav-collapse" role="navigation">
					<ul class="nav">
						<li class="menu-home"><a href="/">Home</a></li>
						<li class="menu-research active active"><a href="/research/">Research</a></li>
						<li class="menu-cv"><a href="/cv/">CV</a></li>
						<li class="menu-news"><a href="/news/">News</a></li>
						<li class="menu-links"><a href="/links/">Links</a></li>
					</ul>
				</nav>
			</div>
		</div>
	</header>

    <div id="wrap" class="container" role="document">
		<div id="content" class="row">
			<!-- publications div -->
			<div id="main" class="span12" role="main">
				<div class="page-header">
					<h1>Publications</h1>
				</div>

				<!-- google scholar & dblp -->
				<div align="center">
					<img src="/images/scholar.png" width="15" height="15" /> <a href="https://scholar.google.com.hk/citations?user=uaKgQSEAAAAJ&hl=en">Google Scholar</a>
					&nbsp;&nbsp;&middot;&nbsp;&nbsp;
					<img src="/images/dblp.png" width="15" height="15" /> <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Yongtao">DBLP</a>
				</div>

				<!-- 2016 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2016</h2>
					</div>
				</div>

				<!-- 16'AAAI look-listen-learn -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="">
							<img class="dropshadow pull-right" alt="" src="/research/publications/look-listen-learn/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="">
							<strong>Look, Listen and Learn - A Multimodal LSTM for Speaker Identiﬁcation</strong>
						</a>
						<br />
						<a href="http://www.jimmyren.com/">Jimmy Ren</a>,
						<b>Yongtao Hu</b>,
						<a href="http://gdriv.es/yuwing">Yu-Wing Tai</a>,
						<a href="http://i.cs.hku.hk/~cwang/">Chuan Wang</a>,
						<a href="http://lxu.me/">Li Xu</a>,
						<a href="https://www.linkedin.com/in/wenxiu-sun-bb6b292b">Wenxiu Sun</a>, and
						<a href="http://www.yan-qiong.com/">Qiong Yan</a>
						<br />
						<em>AAAI 2016</em>
						<br />
						<em>"speaker identification through multimodal weight sharing LSTM"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> project page</li>
							<li><em class="icon-file"></em> paper</li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(ren2016look);">abstract</a>
								<pre id="ren2016look" class="invisible_text">Speaker identification is the task of localizing the face of the person who has the same identity as the ongoing voice in video. This task not only requires collective perception over both visual and auditory signals, the robustness to severe quality degradation and unconstrained content variation is also indispensable. In this paper, we describe a novel multimodal Long Short-Term Memory (LSTM) architecture which seamlessly uniﬁes both visual and auditory modalities from the beginning of each sequence input. The key idea to achieve this is to extend the conventional LSTM by not only sharing weights across time steps, but also sharing weights across modalities. We show that modeling temporal dependency among face frames significantly improves the robustness to content quality degradation and content variation. Modeling multimodal correlations signiﬁcantly improves the robustness to distractors, namely the non-speaking identities. We applied the model in The Big Bang Theory and showed that our system outperformed the state-of-the-art system in speaker identification accuracy while keeping false alarm rate lower.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-briefcase"></em> dataset</li>
							<li><em class="icon-turtle"></em> source code</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(ren2016look2);">bibtex</a>
								<pre id="ren2016look2" class="invisible_text">
@inproceedings{ren2016look,
  title={{Look, Listen and Learn - A Multimodal LSTM for Speaker Identification}},
  author={Ren, Jimmy SJ. and Hu, Yongtao and Tai, Yu-Wing and Wang, Chuan and Xu, Li and Sun, Wenxiu and Yan, Qiong},
  booktitle={Proceedings of the 30th AAAI Conference on Artificial Intelligence},
  pages={xxxx--xxxx},
  year={2016}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> DOI</li>
						</ul>
					</div>
				</div>

				<!-- 2015 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2015</h2>
					</div>
				</div>

				<!-- 15'MM speaker-naming -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/speaker-naming/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/speaker-naming/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/speaker-naming/">
							<strong>Deep Multimodal Speaker Naming</strong>
						</a>
						<br />
						<b>Yongtao Hu</b>,
						<a href="http://www.jimmyren.com/">Jimmy Ren</a>,
						<a href="http://jwdai.net/">Jingwen Dai</a>,
						<a href="http://iris.usc.edu/Vision-Users/OldUsers/cyuan/">Chang Yuan</a>,
						<a href="http://lxu.me/">Li Xu</a>, and
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br />
						<em>ACM MM 2015</em>
						<br />
						<em>"realtime speaker naming through CNN based deeply learned face-audio fusion"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/speaker-naming/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/speaker-naming/paper.pdf">paper</a></li>
							<li><em class="icon-picture"></em> <a href="/research/publications/speaker-naming/poster.pdf">poster</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2015speakernaming);">abstract</a>
								<pre id="hu2015speakernaming" class="invisible_text">Automatic speaker naming is the problem of localizing as well as identifying each speaking character in a TV/movie/live show video. This is a challenging problem mainly attributes to its multimodal nature, namely face cue alone is insufficient to achieve good performance. Previous multimodal approaches to this problem usually process the data of different modalities individually and merge them using handcrafted heuristics. Such approaches work well for simple scenes, but fail to achieve high performance for speakers with large appearance variations. In this paper, we propose a novel convolutional neural networks (CNN) based learning framework to automatically learn the fusion function of both face and audio cues. We show that without using face tracking, facial landmark localization or subtitle/transcript, our system with robust multimodal feature extraction is able to achieve state-of-the-art speaker naming performance evaluated on two diverse TV series. The dataset and implementation of our algorithm are publicly available online.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-briefcase"></em> <a href="/research/publications/speaker-naming/dataset">dataset</a></li>
							<li><em class="icon-turtle"></em> <a href="https://bitbucket.org/herohuyongtao/mm15-speaker-naming">source code</a></li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2015speakernaming2);">bibtex</a>
								<pre id="hu2015speakernaming2" class="invisible_text">
@inproceedings{hu2015deep,
  title={{Deep Multimodal Speaker Naming}},
  author={Hu, Yongtao and Ren, Jimmy SJ. and Dai, Jingwen and Yuan, Chang and Xu, Li and Wang, Wenping},
  booktitle={Proceedings of the 23rd Annual ACM International Conference on Multimedia},
  pages={1107--1110},
  year={2015},
  organization={ACM}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="http://dl.acm.org/citation.cfm?doid=2733373.2806293">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- 15'TMM video2comics -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/video2comics/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/video2comics/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/video2comics/">
							<strong>Content-Aware Video2Comics with Manga-Style Layout</strong>
						</a>
						<br />
						Guangmei Jing,
						<b>Yongtao Hu</b>,
						<a href="http://cs.nju.edu.cn/ywguo/">Yanwen Guo</a>,
						<a href="http://i.cs.hku.hk/~yzyu/">Yizhou Yu</a>, and
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br />
						<em>IEEE TMM 2015</em>
						<br />
						<em>"auto-convert conversational videos into comics with manga-style layout"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/video2comics/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/video2comics/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(jing2015content_abstract);">abstract</a>
								<pre id="jing2015content_abstract" class="invisible_text">We introduce in this paper a new approach that conveniently converts conversational videos into comics with manga-style layout. With our approach, the manga-style layout of a comic page is achieved in a content-driven manner, and the main components, including panels and word balloons, that constitute a visually pleasing comic page are intelligently organized. Our approach extracts key frames on speakers by using a speaker detection technique such that word balloons can be placed near the corresponding speakers. We qualitatively measure the information contained in a comic page. With the initial layout automatically determined, the final comic page is obtained by maximizing such a measure and optimizing the parameters relating to the optimal display of comics. An efficient Markov chain Monte Carlo sampling algorithm is designed for the optimization. Our user study demonstrates that users much prefer our manga-style comics to purely Western style comics. Extensive experiments and comparisons against previous work also verify the effectiveness of our approach.</pre>
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(jing2015content_bibtex);">bibtex</a>
								<pre id="jing2015content_bibtex" class="invisible_text">
@article{jing2015sontent,
  title={{Content-Aware Video2Comics with Manga-Style Layout}},
  author={Jing, Guangmei and Hu, Yongtao and Guo, Yanwen and Yu, Yizhou and Wang, Wenping},
  journal={IEEE Transactions on Multimedia (TMM)},
  volume={PP},
  number={99},
  year={2015},
  publisher={IEEE}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="http://dx.doi.org/10.1109/TMM.2015.2474263">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- 2014 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2014</h2>
					</div>
				</div>

				<!-- 14'TOMM smart-subtitle -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/speaker-following-subtitles/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/speaker-following-subtitles/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/speaker-following-subtitles/">
							<strong>Speaker-Following Video Subtitles</strong>
						</a>
						<br />
						<b>Yongtao Hu</b>,
						<a href="http://www0.cs.ucl.ac.uk/staff/j.kautz/">Jan Kautz</a>,
						<a href="http://i.cs.hku.hk/~yzyu/">Yizhou Yu</a>, and
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br />
						<em>ACM TOMM 2014</em>
						<br />
						<em>"auto-generate speaker-following subtitles to enhance video viewing experience"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/speaker-following-subtitles/">project page</a></li>
							<li><em class="icon-file"></em> <a href="http://arxiv.org/pdf/1407.5145v1.pdf">arXiv (with color)</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/speaker-following-subtitles/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014speakerfollowing2);">abstract</a>
								<pre id="hu2014speakerfollowing2" class="invisible_text">We propose a new method for improving the presentation of subtitles in video (e.g. TV and movies). With conventional subtitles, the viewer has to constantly look away from the main viewing area to read the subtitles at the bottom of the screen, which disrupts the viewing experience and causes unnecessary eyestrain. Our method places on-screen subtitles next to the respective speakers to allow the viewer to follow the visual content while simultaneously reading the subtitles. We use novel identification algorithms to detect the speakers based on audio and visual information. Then the placement of the subtitles is determined using global optimization. A comprehensive usability study indicated that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014speakerfollowing);">bibtex</a>
								<pre id="hu2014speakerfollowing" class="invisible_text">
@article{hu2014speaker,
  title={{Speaker-Following Video Subtitles}},
  author={Hu, Yongtao and Kautz, Jan and Yu, Yizhou and Wang, Wenping},
  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  volume={11},
  number={2},
  pages={32:1--32:17},
  year={2014},
  publisher={ACM}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="http://dx.doi.org/10.1145/2632111">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- thesis -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>Ph.D. Thesis</h2>
					</div>
				</div>

				<!-- phd thesis -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/publications/thesis/thumbnail.png" width="150" height="90" />
					</div>
					<div class="span8">
						<a href="">
							<strong>Multimodal Speaker Localization and Identiﬁcation for Video Processing</strong>
						</a>
						<br />
						<b>Yongtao Hu</b>
						<br />
						<em>The University of Hong Kong, December 2014</em>
						<br />
						<em>"a more-or-less summary of the works published at TOMM'14, TMM'15 and MM'15"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-file"></em> paper</li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014thesis2);">abstract</a>
								<pre id="hu2014thesis2" class="invisible_text">
With the rapid growth of the multimedia data, especially for videos, the ability to better and time-efficiently understand them is becoming increasingly important. For videos, speakers, which are normally what our eyes are focused on, have played a key role to understand the content. With the detailed information of the speakers like their positions and identities, many high-level video processing/analysis tasks, such as semantic indexing, retrieval summarization. Recently, some multimedia content providers, such as Amazon/IMDb and Google Play, had the ability to provide additional cast and characters information for movies and TV series during playback, which can be achieved via a combination of face tracking, automatic identification and crowd sourcing. The main topics includes speaker localization, speaker identification, speech recognition, etc.
<br />This thesis first investigates the problem of speaker localization. A new algorithm for effectively detecting and localizing speakers based on multimodal visual and audio information is presented. We introduce four new features for speaker detection and localization, including lip motion, center contribution, length consistency and audio-visual synchrony, and combine them in a cascade model. Experiments on several movies and TV series indicate that, all together, they improve the speaker detection and localization accuracy by 7.5%--20.5%. Based on the locations of speakers, an efficient optimization algorithm for determining appropriate locations to place subtitles is proposed. This further enables us to develop an automatic end-to-end system for subtitle placement for TV series and movies.
<br />The second part of this thesis studies the speaker identification problem in videos. We propose a novel convolutional neural networks (CNN) based learning framework to automatically learn the fusion function of both faces and audio cues. A systematic multimodal dataset with face and audio samples collected from the real-life videos is created. The high variation of the samples in the dataset, including pose, illumination, facial expression, accessory, occlusion, image quality, scene and aging, wonderfully approximates the realistic scenarios and allows us to fully explore the potential of our method in practical applications. Extensive experiments on our new multi-modal dataset show that our method achieves state-of-the-art performance (over 90%) in speaker naming task without using face/person tracking, facial landmark localization or subtitle/transcript, thus making it suitable for real-life applications.
<br />The speaker-oriented techniques presented in this thesis have lots of applications for video processing. Through extensive experimental results on multiple real-life videos including TV series, movies and online video clips, we demonstrate the ability to extend our previous multimodal speaker localization and speaker identification algorithms in video processing tasks. Particularly, three main categories of applications are introduced, including (1) combine applying our speaker-following video subtitles and speaker naming work to enhance video viewing experience, where a comprehensive usability study with 219 users verifies that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain; (2) automatically convert a video sequence into comics based on our speaker localization algorithms; and (3) extend our speaker naming work to handle real-life video summarization tasks.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014thesis);">bibtex</a>
								<pre id="hu2014thesis" class="invisible_text">
@phdthesis{hu2014thesis,
  title={{Multimodal Speaker Localization and Identification for Video Processing}},
  author={Hu, Yongtao},
  year={2014},
  month={12},
  school={The University of Hong Kong}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
				<p>&nbsp;</p>
			</div><!-- /#main -->

			<!-- projects div -->
			<div id="main" class="span12" role="main">
				<div class="page-header">
					<h1>Research Projects</h1>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/projects/speaker-following-subtitles/">
							<img class="dropshadow pull-right" alt="" src="/research/projects/speaker-following-subtitles/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/projects/speaker-following-subtitles/">
							<strong>New Methodology and Software for Improving Subtitle Presentation in Movies and Videos</strong>
						</a>
						<br />
						<em>07/2014 - 12/2015</em>
						<br />
						<em><a href="https://www.itf.gov.hk/l-eng/ITSP.asp">ITSP project</a> (Project Reference : ITS/226/13) with 1.4M HKD funding.</em>
						<br />
						Sponsor: <a href="http://www.deaf.org.hk/">The Hong Kong Society for the Deaf (HKSFD)</a>.
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/projects/speaker-following-subtitles/">project page</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(subtitle_project);">project summary</a>
								<pre id="subtitle_project" class="invisible_text">
			Subtitles provide valuable aids for understanding conversational speeches in movies and videos. However, the traditional way of placing subtitles at the bottom of the video screen causes eyestrain for ordinary viewers because of the need to constantly move the eyeballs back and forth to follow the speaker expression and the subtitle. The traditional subtitle is also inadequate for people with hearing impairment to understand conversional dialogues in videos.

			A new technology will be developed in this project that improves upon the traditional subtitle presentation in movies and videos. In this improved presentation, subtitles associated with different speakers in a video will be placed right next to the associated speakers, so viewers can better understand what is spoken without having to move the eyeballs too much between the speaker and the subtitle, thus greatly reducing eyestrain. Furthermore, with the aid of the improved subtitle, viewers with hearing impairment can clearly associate the speaker with the spoken contents, therefore enhancing their video viewing experience.

			We shall apply advanced computer vision techniques to face detection and lip motion detection to identify speakers in a video scene. We shall also study the optimal placement of subtitles around an identified speaker. Finally, a prototype software system will be developed that takes a subtitled video in a standard format and produces a new video with the improved subtitle presentation. As indicated by our initial user study, the outcome of this project holds the promise of benefitting all the people viewing a subtitled video and in particular those with hearing difficulty.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>

				<p>&nbsp;</p>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/projects/advanced-fusion/">
							<img class="dropshadow pull-right" alt="" src="/research/projects/advanced-fusion/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/projects/advanced-fusion/">
							<strong>An Advanced Video Fusion System for Security Investigations</strong>
						</a>
						<br />
						<em>06/2012 - 09/2014</em>
						<br />
						<em><a href="https://www.itf.gov.hk/l-eng/ITSP.asp">ITSP project</a> (Project Reference : GHP/060/11) with 5M HKD funding.</em>
						<br />
						Sponsors: <a href="www.microsoft.com/en-hk/">Microsoft HK</a> and <a href="http://www.cyberview.com.hk/">Cyberview</a>.
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/projects/advanced-fusion/">project page</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(fusion_project);">project summary</a>
								<pre id="fusion_project" class="invisible_text">
			Video surveillance is deployed everywhere in HK to enhance public security. Security investigation is still much relied on checking 2D videos from separate camera views.

			This project aims at analysing and fusing videos from multiple cameras to create an informative and easy-to-comprehend reenactment of a past event to assist investigations, providing a global understanding with both space and time registration for complex scenarios and enabling investigators to have a close-up check of actions from novel viewing directions.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>

				<p>&nbsp;</p>
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/projects/infrared-building-modeling/thumbnail.png" width="150" height="90" />
					</div>
					<div class="span8" style="vertical-align: center;">
						<strong>3D Infrared Building Modeling</strong>
						<br />
						<em>03/2011 - 06/2011</em>
						<br />
						Work with <a href="http://hkumea.hku.hk/">HKU-ME</a>.
						<br />
						<em>"3D simulation for visualizing infrared information of buildings"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-facetime-video"></em>
								<a href="http://www.youtube.com/watch?v=a7JBc-R0FMM">video demo #1</a>
							</li>
							<li><em class="icon-facetime-video"></em>
								<a href="http://www.youtube.com/watch?v=1Dy3nZRczdo">video demo #2</a>
							</li>
							<li> <!-- !!!important: should not put toogle texts in first <li>, which will cause weird line breaks of following <li>s -->
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(infrared_building_modeling_project);">project summary</a>
								<pre id="infrared_building_modeling_project" class="invisible_text">
			A system for modeling 3D infrared building of real scenes.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
				<p>&nbsp;</p>
			</div><!-- /#main -->
		</div><!-- /#content -->
    </div><!-- /#wrap -->

	<footer id="content-info" class="container" role="contentinfo">
		<table width="100%">
			<tr>
				<td>
					<p class="copy"><small>&copy; 2014-2015 Yongtao Hu</small></p>
				</td>
				<td style="text-align:center">
					<p class="copy"><small>Last updated on Oct. 30, 2015</small></p>
				</td>
				<td style="text-align:right">
					<a href="http://www3.clustrmaps.com/counter/maps.php?user=a91117ac6" title="Site access statistics">
						<img src="http://SiteStates.com/show/image/31185.jpg" border="0" />
					</a>
				</td>
			</tr>
		</table>
	</footer>

	<!-- From http://stackoverflow.com/a/11668413/72470 -->
	<script>
	  !function ($) {
		$(function(){
		  window.prettyPrint && prettyPrint()
		})
	  }(window.jQuery)
	</script>

</body>
</html>
