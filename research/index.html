<!doctype html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
	<meta charset="utf-8">

	<title>Research | Yongtao Hu</title>

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<script src="/js/vendor/modernizr-2.5.3.min.js"></script>

	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	<script>window.jQuery || document.write('<script src="/js/vendor/jquery-1.7.2.min.js"><\/script>')</script>

	<script src="/js/vendor/spamspan.min.js"></script>
	<script src="/js/vendor/prettify.js"></script>

	<link rel="stylesheet" href="/plugins/social-media-widget/social_widget.css?ver=3.9.1">
	<link rel="stylesheet" href="/glyphicons/css/glyphicons.css">
	<link rel="stylesheet" href="/css/bootstrap.css">
	<link rel="stylesheet" href="/css/bootstrap-responsive.css">
	<link rel="stylesheet" href="/css/app.css">
	<script type='text/javascript' src='/js/plugins.js'></script>
	<script type='text/javascript' src='/js/main.js'></script>
	<link rel="canonical" href="/">
	<link rel="shortcut icon" href="/images/logo.ico">

	<!-- to toggle text -->
	<style type="text/css">
		a.toggle_text_link {
			cursor:pointer;
		}

		pre.invisible_text {
			display: none;
		}
	</style>
	<script language="javascript" type="text/javascript">
		function toggle(element) {
			if(element.style.display=="block") {
				element.style.display="none";
			} else {
				element.style.display="block";
			}
		}
	</script>
</head>

<body class="page page-id-25 page-parent page-template-default top-navbar">
  <!--[if lt IE 7]><div class="alert">Your browser is <em>ancient!</em> <a href="http://browsehappy.com/">Upgrade to a different browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to experience this site.</div><![endif]-->

    <header id="banner" class="navbar navbar-fixed-top" role="banner">
		<div class="navbar-inner">
			<div class="container">
				<a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</a>
				<a class="brand" href="/">
					Yongtao Hu
				</a>
				<nav id="nav-main" class="nav-collapse" role="navigation">
					<ul class="nav">
						<li class="menu-home"><a href="/">Home</a></li>
						<li class="menu-research active active"><a href="/research/">Research</a></li>
						<li class="menu-cv"><a href="/cv/">Resume</a></li>
						<li class="menu-news"><a href="/news/">News</a></li>
						<li class="menu-links"><a href="/links/">Links</a></li>
					</ul>
				</nav>
			</div>
		</div>
	</header>

    <div id="wrap" class="container" role="document">
		<div id="content" class="row">
			<!--======= Publications ========-->
			<div id="main" class="span12" role="main">
				<div class="page-header" id="publications">
					<h1>Publications</h1>
				</div>

				<!-- google scholar & dblp -->
				<div align="center">
					<img src="/images/scholar.png" width="15" height="15" /> <a href="https://scholar.google.com.hk/citations?user=uaKgQSEAAAAJ&hl=en">Google Scholar</a>
					&nbsp;&nbsp;&middot;&nbsp;&nbsp;
					<img src="/images/dblp.png" width="15" height="15" /> <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Yongtao">DBLP</a>
					&nbsp;&nbsp;&middot;&nbsp;&nbsp;
					<img src="/images/ResearchGate.png" width="15" height="15" /> <a href="https://www.researchgate.net/profile/Yongtao_Hu">ResearchGate</a>
				</div>

				<!-- 2020 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2020</h2>
					</div>
				</div>

				<!-- 20'TVCG topo-tag -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/topo-tag/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/topo-tag/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/topo-tag/">
							<strong>TopoTag: A Robust and Scalable Topological Fiducial Marker System</strong>
						</a>
						<br />
						Guoxing Yu,
						<b>Yongtao Hu</b>, and
						<a href="http://jwdai.net/">Jingwen Dai</a>
						<br />
						<em>IEEE Transactions on Visualization and Computer Graphics (<b>TVCG 2020</b>)</em>
						<br />
						<em>"novel fiducial marker system with much improved detection & pose accuracy, scalability and shape flexibility"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/topo-tag/">project page</a></li>
							<li><em class="icon-file"></em> <a href="https://arxiv.org/pdf/1908.01450.pdf">preprint (arXiv)</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/topo-tag/supp_material.pdf">supplemental material</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(yu2020topotag);">abstract</a>
								<pre id="yu2020topotag" class="invisible_text">Fiducial markers have been playing an important role in augmented reality (AR), robot navigation, and general applications where the relative pose between a camera and an object is required. Here we introduce TopoTag, a robust and scalable topological fiducial marker system, which supports reliable and accurate pose estimation from a single image. TopoTag uses topological and geometrical information in marker detection to achieve higher robustness. Topological information is extensively used for 2D marker detection, and further corresponding geometrical information for ID decoding. Robust 3D pose estimation is achieved by taking advantage of all TopoTag vertices. Without sacrificing bits for higher recall and precision like previous systems, TopoTag can use full bits for ID encoding. TopoTag supports tens of thousands unique IDs and easily extends to millions of unique tags resulting in massive scalability. We collected a large test dataset including in total 169,713 images for evaluation, involving in-plane and out-of-plane rotation, image blur, different distances and various backgrounds, etc. Experiments on the dataset and real indoor and outdoor scene tests with a rolling shutter camera both show that TopoTag significantly outperforms previous fiducial marker systems in terms of various metrics, including detection accuracy, vertex jitter, pose jitter and accuracy, etc. In addition, TopoTag supports occlusion as long as the main tag topological structure is maintained and allows for flexible shape design where users can customize internal and external marker shapes. Code for our marker design/generation, marker detection, and dataset are available at https://herohuyongtao.github.io/research/publications/topo-tag/.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-turtle"></em> <a href="/research/publications/topo-tag#code">source code</a></li>
							<li><em class="icon-briefcase"></em> <a href="/research/publications/topo-tag#dataset">dataset</a></li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(yu2020topotag2);">bibtex</a>
								<pre id="yu2020topotag2" class="invisible_text">
@article{yu2020topotag,
  title={{TopoTag: A Robust and Scalable Topological Fiducial Marker System}},
  author={Yu, Guoxing and Hu, Yongtao and Dai, Jingwen},
  journal={IEEE Transactions on Visualization and Computer Graphics (TVCG)},
  volume={},
  number={},
  pages={},
  year={2020},
  publisher={IEEE}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="https://doi.org/	
								10.1109/TVCG.2020.2988466">DOI</a></li>
						</ul>
					</div>
				</div>				

				<!-- 2019 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2019</h2>
					</div>
				</div>

				<!-- 19'ISMAR-Adjunct watch-ar -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/watch-ar/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/watch-ar/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/watch-ar/">
							<strong>WatchAR: 6-DoF Tracked Watch for AR Interaction</strong>
						</a>
						<br />
						<a href="https://zerolu.github.io/">Zhixiong Lu</a>,
						<b>Yongtao Hu</b>, and
						<a href="http://jwdai.net/">Jingwen Dai</a>
						<br />
						<em>2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (<b>ISMAR-Adjunct 2019</b>)</em>
						<br />
						<em>"6-DoF trackable watch with novel AR interactions"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/watch-ar/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/watch-ar/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(lu2019watchar);">abstract</a>
								<pre id="lu2019watchar" class="invisible_text">AR is about to change how people observe and interact with the world. Smart wearable devices are widely used, their input interfaces, like button, rotating bezel, and inertial sensors are good supplementary for interaction. Further 6-DoF information of these wearables will provide richer interaction modalities. We present WatchAR, an interaction system of 6-DoF trackable smartwatch for mobile AR. Three demos demonstrate different interactions: Air Hit shows a way to acquire 2D target with single hand; Picker Input shows how to select an item from a list efficiently; Space Fighter demonstrates the potential of WatchAR for interacting with a game.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(lu2019watchar2);">bibtex</a>
								<pre id="lu2019watchar2" class="invisible_text">
@inproceedings{lu2019watchar,
  title={{WatchAR: 6-DoF Tracked Watch for AR Interaction}},
  author={Lu, Zhixiong and Hu, Yongtao and Dai, Jingwen},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
  pages={},
  year={2019},
  organization={IEEE}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<!--
							<li><em class="icon-chevron-right"></em> <a href="https://doi.org/10.1145/3025453.3025772">DOI</a></li>
							-->
						</ul>
					</div>
				</div>

				<!-- 2017 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2017</h2>
					</div>
				</div>

				<!-- 17'CHI eye-tracking-evaluation -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/eye-tracking-evaluation/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/eye-tracking-evaluation/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/eye-tracking-evaluation/">
							<strong>Close to the Action: Eye-Tracking Evaluation of Speaker-Following Subtitles</strong>
						</a>
						<br />
						<a href="http://www.vis.uni-stuttgart.de/institut/mitarbeiter/kuno-kurzhals.html">Kuno Kurzhals</a>,
						<a href="">Emine Cetinkaya</a>,
						<b>Yongtao Hu</b>,
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>, and
						<a href="http://www.vis.uni-stuttgart.de/~weiskopf/">Daniel Weiskopf</a>
						<br />
						<em>The 35th ACM Conference on Human Factors in Computing Systems (<b>CHI 2017</b>)</em>
						<br />
						<em>"eye-tracking evaluation of speaker-following subtitles"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/eye-tracking-evaluation/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/eye-tracking-evaluation/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(kurzhals2017close);">abstract</a>
								<pre id="kurzhals2017close" class="invisible_text">The incorporation of subtitles in multimedia content plays an important role in communicating spoken content. For example, subtitles in the respective language are often preferred to expensive audio translation of foreign movies. The traditional representation of subtitles displays text centered at the bottom of the screen. This layout can lead to large distances between text and relevant image content, causing eye strain and even that we miss visual content. As a recent alternative, the technique of speaker-following subtitles places subtitle text in speech bubbles close to the current speaker. We conducted a controlled eye-tracking laboratory study (n = 40) to compare the regular approach (center-bottom subtitles) with content-sensitive, speaker-following subtitles. We compared different dialog-heavy video clips with the two layouts. Our results show that speaker-following subtitles lead to higher fixation counts on relevant image regions and reduce saccade length, which is an important factor for eye strain.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(kurzhals2017close2);">bibtex</a>
								<pre id="kurzhals2017close2" class="invisible_text">
@inproceedings{kurzhals2017close,
  title={{Close to the Action: Eye-Tracking Evaluation of Speaker-Following Subtitles}},
  author={Kurzhals, Kuno and Cetinkaya, Emine and Hu, Yongtao and Wang, Wenping and Weiskopf, Daniel},
  booktitle={Proceedings of the 35th ACM Conference on Human Factors in Computing Systems},
  pages={6559--6568},
  year={2017},
  organization={ACM}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="https://doi.org/10.1145/3025453.3025772">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- 2016 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2016</h2>
					</div>
				</div>

				<!-- 16'AAAI look-listen-learn -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/look-listen-learn/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/look-listen-learn/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/look-listen-learn/">
							<strong>Look, Listen and Learn - A Multimodal LSTM for Speaker Identiﬁcation</strong>
						</a>
						<br />
						<a href="http://www.jimmyren.com/">Jimmy SJ. Ren</a>,
						<b>Yongtao Hu</b>,
						<a href="http://gdriv.es/yuwing">Yu-Wing Tai</a>,
						<a href="http://i.cs.hku.hk/~cwang/">Chuan Wang</a>,
						<a href="http://lxu.me/">Li Xu</a>,
						<a href="https://www.linkedin.com/in/wenxiu-sun-bb6b292b">Wenxiu Sun</a>, and
						<a href="http://www.yan-qiong.com/">Qiong Yan</a>
						<br />
						<em>The 30th AAAI Conference on Artificial Intelligence (<b>AAAI 2016</b>)</em>
						<br />
						<em>"speaker identification through multimodal weight sharing LSTM"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/look-listen-learn/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/look-listen-learn/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(ren2016look);">abstract</a>
								<pre id="ren2016look" class="invisible_text">Speaker identiﬁcation refers to the task of localizing the face of a person who has the same identity as the ongoing voice in a video. This task not only requires collective perception over both visual and auditory signals, the robustness to handle severe quality degradations and unconstrained content variations are also indispensable. In this paper, we describe a novel multimodal Long Short-Term Memory (LSTM) architecture which seamlessly uniﬁes both visual and auditory modalities from the beginning of each sequence input. The key idea is to extend the conventional LSTM by not only sharing weights across time steps, but also sharing weights across modalities. We show that modeling the temporal dependency across face and voice can signiﬁcantly improve the robustness to content quality degradations and variations. We also found that our multimodal LSTM is robustness to distractors, namely the non-speaking identities. We applied our multimodal LSTM to The Big Bang Theory dataset and showed that our system outperforms the state-of-the-art systems in speaker identification with lower false alarm rate and higher recognition accuracy.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-turtle"></em> <a href="https://github.com/jimmy-ren/lstm_speaker_naming_aaai16">source code & dataset</a></li>
							<li><em class="icon-turtle"></em> <a href="https://github.com/jimmy-ren/vLSTM">more applications</a></li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(ren2016look2);">bibtex</a>
								<pre id="ren2016look2" class="invisible_text">
@inproceedings{ren2016look,
  title={{Look, Listen and Learn - A Multimodal LSTM for Speaker Identification}},
  author={Ren, Jimmy SJ. and Hu, Yongtao and Tai, Yu-Wing and Wang, Chuan and Xu, Li and Sun, Wenxiu and Yan, Qiong},
  booktitle={Proceedings of the 30th AAAI Conference on Artificial Intelligence},
  pages={3581--3587},
  year={2016}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="http://dl.acm.org/citation.cfm?id=3016407">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- 2015 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2015</h2>
					</div>
				</div>

				<!-- 15'MM speaker-naming -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/speaker-naming/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/speaker-naming/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/speaker-naming/">
							<strong>Deep Multimodal Speaker Naming</strong>
						</a>
						<br />
						<b>Yongtao Hu</b>,
						<a href="http://www.jimmyren.com/">Jimmy SJ. Ren</a>,
						<a href="http://jwdai.net/">Jingwen Dai</a>,
						<a href="http://iris.usc.edu/Vision-Users/OldUsers/cyuan/">Chang Yuan</a>,
						<a href="http://lxu.me/">Li Xu</a>, and
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br />
						<em>The 23rd Annual ACM International Conference on Multimedia (<b>MM 2015</b>)</em>
						<br />
						<em>"realtime speaker naming through CNN based deeply learned face-audio fusion"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/speaker-naming/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/speaker-naming/paper.pdf">paper</a></li>
							<li><em class="icon-picture"></em> <a href="/research/publications/speaker-naming/poster.pdf">poster</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2015speakernaming);">abstract</a>
								<pre id="hu2015speakernaming" class="invisible_text">Automatic speaker naming is the problem of localizing as well as identifying each speaking character in a TV/movie/live show video. This is a challenging problem mainly attributes to its multimodal nature, namely face cue alone is insufficient to achieve good performance. Previous multimodal approaches to this problem usually process the data of different modalities individually and merge them using handcrafted heuristics. Such approaches work well for simple scenes, but fail to achieve high performance for speakers with large appearance variations. In this paper, we propose a novel convolutional neural networks (CNN) based learning framework to automatically learn the fusion function of both face and audio cues. We show that without using face tracking, facial landmark localization or subtitle/transcript, our system with robust multimodal feature extraction is able to achieve state-of-the-art speaker naming performance evaluated on two diverse TV series. The dataset and implementation of our algorithm are publicly available online.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-briefcase"></em> <a href="/research/publications/speaker-naming/dataset">dataset</a></li>
							<li><em class="icon-turtle"></em> <a href="https://bitbucket.org/herohuyongtao/mm15-speaker-naming">source code</a></li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2015speakernaming2);">bibtex</a>
								<pre id="hu2015speakernaming2" class="invisible_text">
@inproceedings{hu2015deep,
  title={{Deep Multimodal Speaker Naming}},
  author={Hu, Yongtao and Ren, Jimmy SJ. and Dai, Jingwen and Yuan, Chang and Xu, Li and Wang, Wenping},
  booktitle={Proceedings of the 23rd Annual ACM International Conference on Multimedia},
  pages={1107--1110},
  year={2015},
  organization={ACM}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="https://doi.org/10.1145/2733373.2806293">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- 15'TMM video2comics -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/video2comics/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/video2comics/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/video2comics/">
							<strong>Content-Aware Video2Comics with Manga-Style Layout</strong>
						</a>
						<br />
						<a href="https://www.linkedin.com/in/guangmei-jing-04917a49">Guangmei Jing</a>,
						<b>Yongtao Hu</b>,
						<a href="http://cs.nju.edu.cn/ywguo/">Yanwen Guo</a>,
						<a href="http://i.cs.hku.hk/~yzyu/">Yizhou Yu</a>, and
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br />
						<em>IEEE Transactions on Multimedia (<b>TMM 2015</b>)</em>
						<br />
						<em>"auto-convert conversational videos into comics with manga-style layout"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/video2comics/">project page</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/video2comics/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(jing2015content_abstract);">abstract</a>
								<pre id="jing2015content_abstract" class="invisible_text">We introduce in this paper a new approach that conveniently converts conversational videos into comics with manga-style layout. With our approach, the manga-style layout of a comic page is achieved in a content-driven manner, and the main components, including panels and word balloons, that constitute a visually pleasing comic page are intelligently organized. Our approach extracts key frames on speakers by using a speaker detection technique such that word balloons can be placed near the corresponding speakers. We qualitatively measure the information contained in a comic page. With the initial layout automatically determined, the final comic page is obtained by maximizing such a measure and optimizing the parameters relating to the optimal display of comics. An efficient Markov chain Monte Carlo sampling algorithm is designed for the optimization. Our user study demonstrates that users much prefer our manga-style comics to purely Western style comics. Extensive experiments and comparisons against previous work also verify the effectiveness of our approach.</pre>
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(jing2015content_bibtex);">bibtex</a>
								<pre id="jing2015content_bibtex" class="invisible_text">
@article{jing2015sontent,
  title={{Content-Aware Video2Comics with Manga-Style Layout}},
  author={Jing, Guangmei and Hu, Yongtao and Guo, Yanwen and Yu, Yizhou and Wang, Wenping},
  journal={IEEE Transactions on Multimedia (TMM)},
  volume={17},
  number={12},
  pages={2122--2133},
  year={2015},
  publisher={IEEE}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="https://doi.org/10.1109/TMM.2015.2474263">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- 2014 publications -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>2014</h2>
					</div>
				</div>

				<!-- 14'TOMM smart-subtitle -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/publications/speaker-following-subtitles/">
							<img class="dropshadow pull-right" alt="" src="/research/publications/speaker-following-subtitles/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/publications/speaker-following-subtitles/">
							<strong>Speaker-Following Video Subtitles</strong>
						</a>
						<br />
						<b>Yongtao Hu</b>,
						<a href="http://jankautz.com/">Jan Kautz</a>,
						<a href="http://i.cs.hku.hk/~yzyu/">Yizhou Yu</a>, and
						<a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
						<br />
						<em>ACM Transactions on Multimedia Computing, Communications, and Applications (<b>TOMM 2014</b>)</em>
						<br />
						<em>"auto-generate speaker-following subtitles to enhance video viewing experience"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/publications/speaker-following-subtitles/">project page</a></li>
							<li><em class="icon-file"></em> <a href="http://arxiv.org/pdf/1407.5145v1.pdf">arXiv (with color)</a></li>
							<li><em class="icon-file"></em> <a href="/research/publications/speaker-following-subtitles/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014speakerfollowing2);">abstract</a>
								<pre id="hu2014speakerfollowing2" class="invisible_text">We propose a new method for improving the presentation of subtitles in video (e.g. TV and movies). With conventional subtitles, the viewer has to constantly look away from the main viewing area to read the subtitles at the bottom of the screen, which disrupts the viewing experience and causes unnecessary eyestrain. Our method places on-screen subtitles next to the respective speakers to allow the viewer to follow the visual content while simultaneously reading the subtitles. We use novel identification algorithms to detect the speakers based on audio and visual information. Then the placement of the subtitles is determined using global optimization. A comprehensive usability study indicated that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014speakerfollowing);">bibtex</a>
								<pre id="hu2014speakerfollowing" class="invisible_text">
@article{hu2014speaker,
  title={{Speaker-Following Video Subtitles}},
  author={Hu, Yongtao and Kautz, Jan and Yu, Yizhou and Wang, Wenping},
  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  volume={11},
  number={2},
  pages={32:1--32:17},
  year={2014},
  publisher={ACM}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="https://doi.org/10.1145/2632111">DOI</a></li>
						</ul>
					</div>
				</div>

				<!-- thesis -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span6 offset3">
						<h2>Dissertations</h2>
					</div>
				</div>

				<!-- phd thesis -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/publications/thesis/hku.png" width="150" height="72" />
					</div>
					<div class="span8">
						<strong>Multimodal Speaker Localization and Identiﬁcation for Video Processing</strong>
						<br />
						<em>Ph.D. Thesis, The University of Hong Kong, December 2014</em>
						<br />
						<em>"a more-or-less summary of the works published at TOMM'14, TMM'15 and MM'15"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="/research/publications/thesis/paper.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014thesis2);">abstract</a>
								<pre id="hu2014thesis2" class="invisible_text">
With the rapid growth of the multimedia data, especially for videos, the ability to better and time-efficiently understand them is becoming increasingly important. For videos, speakers, which are normally what our eyes are focused on, have played a key role to understand the content. With the detailed information of the speakers like their positions and identities, many high-level video processing/analysis tasks, such as semantic indexing, retrieval summarization. Recently, some multimedia content providers, such as Amazon/IMDb and Google Play, had the ability to provide additional cast and characters information for movies and TV series during playback, which can be achieved via a combination of face tracking, automatic identification and crowd sourcing. The main topics includes speaker localization, speaker identification, speech recognition, etc.
<br />This thesis first investigates the problem of speaker localization. A new algorithm for effectively detecting and localizing speakers based on multimodal visual and audio information is presented. We introduce four new features for speaker detection and localization, including lip motion, center contribution, length consistency and audio-visual synchrony, and combine them in a cascade model. Experiments on several movies and TV series indicate that, all together, they improve the speaker detection and localization accuracy by 7.5%--20.5%. Based on the locations of speakers, an efficient optimization algorithm for determining appropriate locations to place subtitles is proposed. This further enables us to develop an automatic end-to-end system for subtitle placement for TV series and movies.
<br />The second part of this thesis studies the speaker identification problem in videos. We propose a novel convolutional neural networks (CNN) based learning framework to automatically learn the fusion function of both faces and audio cues. A systematic multimodal dataset with face and audio samples collected from the real-life videos is created. The high variation of the samples in the dataset, including pose, illumination, facial expression, accessory, occlusion, image quality, scene and aging, wonderfully approximates the realistic scenarios and allows us to fully explore the potential of our method in practical applications. Extensive experiments on our new multi-modal dataset show that our method achieves state-of-the-art performance (over 90%) in speaker naming task without using face/person tracking, facial landmark localization or subtitle/transcript, thus making it suitable for real-life applications.
<br />The speaker-oriented techniques presented in this thesis have lots of applications for video processing. Through extensive experimental results on multiple real-life videos including TV series, movies and online video clips, we demonstrate the ability to extend our previous multimodal speaker localization and speaker identification algorithms in video processing tasks. Particularly, three main categories of applications are introduced, including (1) combine applying our speaker-following video subtitles and speaker naming work to enhance video viewing experience, where a comprehensive usability study with 219 users verifies that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain; (2) automatically convert a video sequence into comics based on our speaker localization algorithms; and (3) extend our speaker naming work to handle real-life video summarization tasks.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014thesis);">bibtex</a>
								<pre id="hu2014thesis" class="invisible_text">
@phdthesis{hu2014thesis,
  title={{Multimodal Speaker Localization and Identification for Video Processing}},
  author={Hu, Yongtao},
  year={2014},
  month={12},
  school={The University of Hong Kong}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-chevron-right"></em> <a href="https://doi.org/10.5353/th_b5543983">DOI</a></li>
						</ul>
					</div>
				</div>
				<p>&nbsp;</p>

				<!-- bs thesis -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/publications/thesis/sdu.png" width="150" height="72" />
					</div>
					<div class="span8">
						<strong>A New Image Blending Method based on Poisson Editing</strong>
						<br />
						<em>B.S. Thesis (in Chinese), Shandong University, March 2010</em>
						<br />
						<em>"image blending via combining GrabCut and Poisson"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-file"></em> <a href="/research/publications/thesis/paper_sdu.pdf">paper</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014thesis2_sdu);">abstract</a>
								<pre id="hu2014thesis2_sdu" class="invisible_text">
An image is said to be worth thousands of words. However, in reality, people are not always satisfied with existed images. That’s why image blending comes to its life. Its goal is first to extract target objects from source images, and then to embed them to the target image. Nowadays, there are three popular methods as follows: one is the method used in Adobe Photoshop (not yet been published); the second is to blend two images through constructing Laplacian pyramid and doing interpolation; the third is ‘Poisson Image Editing’, which has been the basis of many blending algorithms in recent years. Although ‘Poisson Image Editing’ has concise expressions and is easy to understand and compute, its biggest weakness is that it has to rely on the extracted contour by users manually. Because it needs that users can draw the contour even for some complex images, it really gives users a great deal of inconvenience.
<br />This paper presents a new way based on ‘Poisson Image Editing’ to do image blending. Its goal is to make users free of the tedious task of manually extracting images’ contours and improve the quality of integration. This new way of image blending consists of three main processes. Firstly, I will extract the to-be blended areas or objects (areas or objects interested) from the to-be blended images. Secondly, I can obtain the exact contours of the to-be blended areas or objects based on the first step. Thirdly, I will apply Poisson blending to do image blending based on the exact contours of the to-be blended areas or objects. The core of this new way of image blending is still Poisson blending, so it itself has the advantage of handling illumination changes between images which is the immanent advantage of Poisson blending. What’s more, and what is important is that, doing GrabCut and getting the exact contours first overcome the bad results of the blending boundary areas, because in traditional Poisson blending, the blending contours are too big. This in all makes the new way of image blending can better handle texture or color differences and produce preferable pleased results.
</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li>
								<em class="icon-bookmark"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(hu2014thesis_sdu);">bibtex</a>
								<pre id="hu2014thesis_sdu" class="invisible_text">
@phdthesis{hu2010thesis,
  title={{A New Image Blending Method based on Poisson Editing}},
  author={Hu, Yongtao},
  year={2010},
  month={3},
  school={Shandong University},
  type={Bachelor's Thesis}
}</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
				<p>&nbsp;</p>

				<!--======= Projects ========-->
				<div class="page-header" id="projects">
					<h1>Projects</h1>
				</div>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/projects/speaker-following-subtitles/">
							<img class="dropshadow pull-right" alt="" src="/research/projects/speaker-following-subtitles/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/projects/speaker-following-subtitles/">
							<strong>New Methodology and Software for Improving Subtitle Presentation in Movies and Videos</strong>
						</a>
						<br />
						<em>07/2014 - 12/2015</em>
						<br />
						<em><a href="https://www.itf.gov.hk/l-eng/ITSP.asp">ITSP project</a> (Project Reference : ITS/226/13) with 1.4M HKD funding.</em>
						<br />
						Sponsor: <a href="http://www.deaf.org.hk/">The Hong Kong Society for the Deaf (HKSFD)</a>.
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/projects/speaker-following-subtitles/">project page</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(subtitle_project);">project summary</a>
								<pre id="subtitle_project" class="invisible_text">
Subtitles provide valuable aids for understanding conversational speeches in movies and videos. However, the traditional way of placing subtitles at the bottom of the video screen causes eyestrain for ordinary viewers because of the need to constantly move the eyeballs back and forth to follow the speaker expression and the subtitle. The traditional subtitle is also inadequate for people with hearing impairment to understand conversional dialogues in videos.

A new technology will be developed in this project that improves upon the traditional subtitle presentation in movies and videos. In this improved presentation, subtitles associated with different speakers in a video will be placed right next to the associated speakers, so viewers can better understand what is spoken without having to move the eyeballs too much between the speaker and the subtitle, thus greatly reducing eyestrain. Furthermore, with the aid of the improved subtitle, viewers with hearing impairment can clearly associate the speaker with the spoken contents, therefore enhancing their video viewing experience.

We shall apply advanced computer vision techniques to face detection and lip motion detection to identify speakers in a video scene. We shall also study the optimal placement of subtitles around an identified speaker. Finally, a prototype software system will be developed that takes a subtitled video in a standard format and produces a new video with the improved subtitle presentation. As indicated by our initial user study, the outcome of this project holds the promise of benefitting all the people viewing a subtitled video and in particular those with hearing difficulty.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
				<p>&nbsp;</p>

				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<a href="/research/projects/advanced-fusion/">
							<img class="dropshadow pull-right" alt="" src="/research/projects/advanced-fusion/thumbnail.png" width="150" height="90" />
						</a>
					</div>
					<div class="span8">
						<a href="/research/projects/advanced-fusion/">
							<strong>An Advanced Video Fusion System for Security Investigations</strong>
						</a>
						<br />
						<em>06/2012 - 09/2014</em>
						<br />
						<em><a href="https://www.itf.gov.hk/l-eng/ITSP.asp">ITSP project</a> (Project Reference : GHP/060/11) with 5M HKD funding.</em>
						<br />
						Sponsors: <a href="http://www.microsoft.com/en-hk/">Microsoft HK</a> and <a href="http://www.cyberview.com.hk/">Cyberview</a>.
						<br />
						<ul class="inline middot">
							<li><em class="icon-home"></em> <a href="/research/projects/advanced-fusion/">project page</a></li>
							<li>
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(fusion_project);">project summary</a>
								<pre id="fusion_project" class="invisible_text">
Video surveillance is deployed everywhere in HK to enhance public security. Security investigation is still much relied on checking 2D videos from separate camera views.

This project aims at analysing and fusing videos from multiple cameras to create an informative and easy-to-comprehend reenactment of a past event to assist investigations, providing a global understanding with both space and time registration for complex scenarios and enabling investigators to have a close-up check of actions from novel viewing directions.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
							<li><em class="icon-projector"></em> <a href="http://hku.hk/press/press-releases/detail/11366.html">media</a></li>
						</ul>
					</div>
				</div>
				<p>&nbsp;</p>
				
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/projects/infrared-building-modeling/thumbnail.png" width="150" height="90" />
					</div>
					<div class="span8" style="vertical-align: center;">
						<strong>3D Infrared Building Modeling</strong>
						<br />
						<em>03/2011 - 06/2011</em>
						<br />
						Work with <a href="http://hkumea.hku.hk/">HKU-ME</a>.
						<br />
						<em>"3D simulation for visualizing infrared information of buildings"</em>
						<br />
						<ul class="inline middot">
							<li><em class="icon-facetime-video"></em>
								<a href="http://www.youtube.com/watch?v=a7JBc-R0FMM">video demo #1</a>
							</li>
							<li><em class="icon-facetime-video"></em>
								<a href="http://www.youtube.com/watch?v=1Dy3nZRczdo">video demo #2</a>
							</li>
							<li> <!-- !!!important: should not put toogle texts in first <li>, which will cause weird line breaks of following <li>s -->
								<em class="icon-circle-info"></em>
								<a class="toggle_text_link" onclick="javascript:toggle(infrared_building_modeling_project);">project summary</a>
								<pre id="infrared_building_modeling_project" class="invisible_text">
A system for modeling 3D infrared building of real scenes.</pre> <!-- put </pre> on this line to avoid new unwanted line breaks after bibtex -->
							</li>
						</ul>
					</div>
				</div>
				<p>&nbsp;</p>


				<!--======= Patents ========-->
				<div class="page-header" id="patents">
					<h1>Patents</h1>
				</div>

				<!- - US20200126267 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US20200126267/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Method of Controlling Virtual Content, Terminal Device and Computer Readable Medium</strong>
						<br />
						Yiqun Wu, <b>Yongtao Hu</b>, Jingwen Dai, and Jie He
						<br />
						<em><a href="https://patents.google.com/patent/US20200126267A1">US Patent 20200126267</a>, filed on Dec. 19, 2019</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>					

				<!- - US20200090365 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US20200090365/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Method of Device Tracking, Terminal Device, and Storage Medium</strong>
						<br />
						<b>Yongtao Hu</b>, Guoxing Yu, and Jingwen Dai
						<br />
						<em><a href="https://patents.google.com/patent/US20200090365A1">US Patent 20200090365</a>, filed on Nov. 19, 2019</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>		

				<!- - US20200043242 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US20200043242/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Interactive Method for Virtual Content and Terminal Device</strong>
						<br />
						<b>Yongtao Hu</b>, Sibin Huang, Jingwen Dai, and Jie He
						<br />
						<em><a href="https://patents.google.com/patent/US20200043242A1">US Patent 20200043242</a>, filed on Oct. 14, 2019</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>		

				<!--
				<!- - WO2019154169 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/WO2019154169/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Method for Tracking Interactive Apparatus, and Storage Medium and Electronic Device</strong>
						<br />
						<b>Yongtao Hu</b>, Jingwen Dai, and Jie He
						<br />
						<em>WO Patent 2019154169, filed on Jan. 29, 2019</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>		
				-->			

				<!- - US20190137276 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US20190137276/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Method and Device for Aligning Coordinate of Position Device with Coordinate of IMU</strong>
						<br />
						<b>Yongtao Hu</b>, Jingwen Dai, and Jie He
						<br />
						<em><a href="https://patents.google.com/patent/US20190137276A1">US Patent 20190137276</a>, filed on Dec. 29, 2018</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>					

				<!--
				<!- - WO2019153971 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/WO2019153971/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Visual Interaction Apparatus and Marker</strong>
						<br />
						Jingwen Dai, Jun Huang, Bisheng Rao, Jie He, and <b>Yongtao Hu</b>
						<br />
						<em>WO Patent 2019153971, filed on Dec. 29, 2018</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>		
				-->							

				<!- - US20190034720 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US20190034720/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Method and Device for Searching Stripe Set</strong>
						<br />
						Jie He, Jingwen Dai, Congling Wan, and <b>Yongtao Hu</b>
						<br />
						<em><a href="https://patents.google.com/patent/US20190034720A1">US Patent 20190034720</a>, filed on Sep. 21, 2018</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>	
				
				<!-- US10402988 -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US10402988/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Image Processing Apparatuses and Methods</strong>
						<br />
						Jie He, Jingwen Dai, Congling Wan, and <b>Yongtao Hu</b>
						<br />
						<em><a href="https://patents.google.com/patent/US10402988B2">US Patent 10402988</a>, filed on Dec. 12, 2017, granted on Sep. 3, 2019</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>					

				<!- - US20190385340 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US20190385340/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Method, Device and System for Identifying Light Spot</strong>
						<br />
						<b>Yongtao Hu</b>, Jingwen Dai, and Jie He
						<br />
						<em><a href="https://patents.google.com/patent/US20190385340A1">US Patent 20190385340</a>, filed on Aug. 29, 2017</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>	

				<!-- US10319100 -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US10319100/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Methods, Devices, and Systems for Identifying and Tracking an Object with Multiple Cameras
						</strong>
						<br />
						Jingwen Dai, <b>Yongtao Hu</b>, and Jie He
						<br />
						<em><a href="https://patents.google.com/patent/US10319100B2">US Patent 10319100</a>, filed on Aug. 4, 2017, granted on Jun. 11, 2019</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>				

				<!-- US10347002 -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US10347002/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Electronic Tracking Device, Electronic Tracking System and Electronic Tracking Method
						</strong>
						<br />
						Jingwen Dai, <b>Yongtao Hu</b>, and Jie He
						<br />
						<em><a href="https://patents.google.com/patent/US10347002B2">US Patent 10347002</a>, filed on Jun. 28, 2017, granted on Jul. 9, 2019</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>					

				<!- - US20190392590 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US20190392590/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Method and Device for Identifying Light Source</strong>
						<br />
						<b>Yongtao Hu</b>, Jingwen Dai, and Jie He
						<br />
						<em><a href="https://patents.google.com/patent/US20190392590A1">US Patent 20190392590</a>, filed on Jun. 21, 2017</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>	

				<!--
				<!- - WO2018010149 - ->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/WO2018010149/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Method and Apparatus for Identifying Flickering Light Source</strong>
						<br />
						<b>Yongtao Hu</b>, Jingwen Dai, and Jie He
						<br />
						<em>WO Patent 2018010149, filed on Jul. 14, 2016</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>				
				-->	
				
				<!-- US20180173327 -->
				<div class="row" style="margin-bottom: 1em;">
					<div class="span2 offset1">
						<img class="dropshadow pull-right" alt="" src="/research/patents/US20180173327/thumbnail.png" width="150" height="54" />
					</div>
					<div class="span8">
						<strong>Method, Device and Terminal for Determining Effectiveness of Stripe Set</strong>
						<br />
						Jie He, Jingwen Dai, Congling Wan, and <b>Yongtao Hu</b>
						<br />
						<em><a href="https://patents.google.com/patent/US20180173327A1">US Patent 20180173327</a>, filed on Mar. 22, 2016</em>
						<br />
					</div>
				</div>
				<p>&nbsp;</p>														
			</div><!-- /#main -->
		</div><!-- /#content -->
    </div><!-- /#wrap -->

	<footer id="content-info" class="container" role="contentinfo">
		<table width="100%">
			<tr>
				<td>
					<p class="copy"><small>&copy; 2020 Yongtao Hu</small></p>
				</td>
				<td style="text-align:center">
					<p class="copy"><small>Last updated: 05/2020</small></p>
				</td>
				<td style="text-align:right">
					<a href="https://SiteStates.com" title="Site access statistics">
						<img src="https://SiteStates.com/show/image/31185.jpg" border="0" />
					</a>
				</td>
			</tr>
		</table>
	</footer>

	<!-- From http://stackoverflow.com/a/11668413/72470 -->
	<script>
	  !function ($) {
		$(function(){
		  window.prettyPrint && prettyPrint()
		})
	  }(window.jQuery)
	</script>

</body>
</html>
